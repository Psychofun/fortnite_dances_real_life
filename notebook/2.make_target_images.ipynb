{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# take a video on webcam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "cd ../src/utils\n",
    "python save_img.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Frames from Video File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "save_dir = Path('../data/target/')\n",
    "try: \n",
    "    save_dir.mkdir(exist_ok=True)\n",
    "except FileExistsError : \n",
    "    print(\"Directory exists. Abort creation\")\n",
    "    \n",
    "\n",
    "\n",
    "img_dir = save_dir.joinpath('images')\n",
    "try:\n",
    "    img_dir.mkdir(exist_ok=True)\n",
    "except FileExostsError:\n",
    "    print(\"Directory exists. Abort creation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "import numpy as np\n",
    "\n",
    "def extract_images_from_video(filepath, subsample =  1):\n",
    "    \"\"\"\n",
    "    filepath: string\n",
    "        file path to video.\n",
    "    subsample: int \n",
    "        use only one of subsample images.\n",
    "        subsample = 1, use all images.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Video path: \",str(save_dir.joinpath(filepath)))\n",
    "    cap = cv2.VideoCapture(str(save_dir.joinpath(filepath)))\n",
    "    crop = True\n",
    "    i = 0\n",
    "    j = 0\n",
    "    flag = None\n",
    "    while(cap.isOpened()):\n",
    "        #Extract only 1 of subsample images\n",
    "        flag, frame = cap.read()\n",
    "        if flag == False:\n",
    "            break\n",
    "        if j%subsample == 0:\n",
    "\n",
    "            # Crop images to desired size \n",
    "            if crop:\n",
    "                frame = frame[355:-10,645:-645,:]\n",
    "            #frame = cv2.resize(frame, dsize=(512, 512), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "            #if i >= 10:\n",
    "            #    break\n",
    "\n",
    "            cv2.imwrite(str(img_dir.joinpath('img_{:05d}.png'.format(i))), frame)\n",
    "            i += 1\n",
    "\n",
    "\n",
    "        j+= 1\n",
    "    print(\"Number of frames:\", i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pose estimation (OpenPose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "openpose_dir = Path('../src/pytorch_Realtime_Multi-Person_Pose_Estimation/')\n",
    "\n",
    "import sys\n",
    "sys.path.append(str(openpose_dir))\n",
    "sys.path.append('../src/utils')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autorelaod modules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# openpose\n",
    "from network.rtpose_vgg import get_model\n",
    "from evaluate.coco_eval import get_multiplier, get_outputs\n",
    "\n",
    "# utils\n",
    "from openpose_utils import remove_noise, get_pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bulding VGG19\n"
     ]
    }
   ],
   "source": [
    "weight_name = openpose_dir.joinpath('network/weight/pose_model.pth')\n",
    "\n",
    "model = get_model('vgg19')     \n",
    "model.load_state_dict(torch.load(weight_name))\n",
    "model = torch.nn.DataParallel(model).cuda()\n",
    "model.float()\n",
    "model.eval()\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img_path = sorted(img_dir.iterdir())[0]\n",
    "img = cv2.imread(str(img_path))\n",
    "shape_dst = np.min(img.shape[:2])\n",
    "# offset\n",
    "oh = (img.shape[0] - shape_dst) // 2\n",
    "ow = (img.shape[1] - shape_dst) // 2\n",
    "\n",
    "img = img[oh:oh+shape_dst, ow:ow+shape_dst]\n",
    "img = cv2.resize(img, (512, 512))\n",
    "          \n",
    "plt.imshow(img[:,:,[2, 1, 0]]) # BGR -> RGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiplier = get_multiplier(img)\n",
    "with torch.no_grad():\n",
    "    paf, heatmap = get_outputs(multiplier, img, model, 'rtpose')\n",
    "    \n",
    "r_heatmap = np.array([remove_noise(ht)\n",
    "                      for ht in heatmap.transpose(2, 0, 1)[:-1]])\\\n",
    "                     .transpose(1, 2, 0)\n",
    "heatmap[:, :, :-1] = r_heatmap\n",
    "param = {'thre1': 0.1, 'thre2': 0.05, 'thre3': 0.5}\n",
    "label = get_pose(param, heatmap, paf)\n",
    "\n",
    "plt.imshow(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make label images for pix2pix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = save_dir.joinpath('train')\n",
    "try:\n",
    "    train_dir.mkdir(exist_ok=True)\n",
    "except FileExistsError:\n",
    "    print(\"Directory exists. Abort creation\")\n",
    "    \n",
    "\n",
    "train_img_dir = train_dir.joinpath('train_img')\n",
    "train_img_dir.mkdir(exist_ok=True)\n",
    "train_label_dir = train_dir.joinpath('train_label')\n",
    "train_label_dir.mkdir(exist_ok=True)\n",
    "\n",
    "for idx in tqdm(range(20)):\n",
    "    img_path = img_dir.joinpath('img_{:05d}.png'.format(idx))\n",
    "    img = cv2.imread(str(img_path))\n",
    "    shape_dst = np.min(img.shape[:2])\n",
    "    oh = (img.shape[0] - shape_dst) // 2\n",
    "    ow = (img.shape[1] - shape_dst) // 2\n",
    "\n",
    "    img = img[oh:oh+shape_dst, ow:ow+shape_dst]\n",
    "    img = cv2.resize(img, (512, 512))\n",
    "    multiplier = get_multiplier(img)\n",
    "    with torch.no_grad():\n",
    "        paf, heatmap = get_outputs(multiplier, img, model, 'rtpose')\n",
    "    r_heatmap = np.array([remove_noise(ht)\n",
    "                      for ht in heatmap.transpose(2, 0, 1)[:-1]])\\\n",
    "                     .transpose(1, 2, 0)\n",
    "    heatmap[:, :, :-1] = r_heatmap\n",
    "    param = {'thre1': 0.1, 'thre2': 0.05, 'thre3': 0.5}\n",
    "    label = get_pose(param, heatmap, paf)\n",
    "    \n",
    "    cv2.imwrite(str(train_img_dir.joinpath('img_{:05d}.png'.format(idx))), img)    \n",
    "    cv2.imwrite(str(train_label_dir.joinpath('label_{:05d}.png'.format(idx))), label)\n",
    "    \n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "privateai",
   "language": "python",
   "name": "privateai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
